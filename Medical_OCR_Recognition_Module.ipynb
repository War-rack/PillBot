{"cells":[{"cell_type":"markdown","metadata":{"id":"sqR-fSou0DaI"},"source":["## Imports"]},{"cell_type":"code","source":["!pip install thefuzz\n","!pip install thefuzz[speedup]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hGd2oAS5dzE","executionInfo":{"status":"ok","timestamp":1741088071374,"user_tz":-330,"elapsed":19784,"user":{"displayName":"Pranaya Jandial","userId":"08091217684900804801"}},"outputId":"6332f710-c614-4fd6-c74d-40b1c302bc3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thefuzz\n","  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n","  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n","Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, thefuzz\n","Successfully installed rapidfuzz-3.12.2 thefuzz-0.22.1\n","Requirement already satisfied: thefuzz[speedup] in /usr/local/lib/python3.11/dist-packages (0.22.1)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from thefuzz[speedup]) (3.12.2)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPCKRm9O0DaJ"},"outputs":[],"source":["from tensorflow.keras.layers import StringLookup\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","from thefuzz import process\n","from thefuzz import fuzz\n","\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96110,"status":"ok","timestamp":1741088174602,"user":{"displayName":"Pranaya Jandial","userId":"08091217684900804801"},"user_tz":-330},"id":"hIcUUQa07OGu","outputId":"caadf0c1-bf36-4f68-931b-1748c80289ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CkUECYg40DaK"},"source":["\n","\n","## Importing IAM Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5s9INtI60DaK","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1741088185589,"user_tz":-330,"elapsed":883,"user":{"displayName":"Pranaya Jandial","userId":"08091217684900804801"}},"outputId":"b90808fb-636e-4c5d-8f79-770cc4b4a2dd","collapsed":true},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'drive/MyDrive/Medical_OCR/data/lines.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1147659f982d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlines_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base_path}/data/lines.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Medical_OCR/data/lines.txt'"]}],"source":["base_path = \"drive/MyDrive/Medical_OCR\"\n","lines_list = []\n","\n","lines = open(f\"{base_path}/data/lines.txt\", \"r\").readlines()\n","for line in lines:\n","    if line[0] == \"#\":\n","        continue\n","    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n","        lines_list.append(line)\n","\n","np.random.shuffle(lines_list)\n","len(lines_list)"]},{"cell_type":"code","source":["lines_aug_list = []\n","\n","lines_aug = open(f\"{base_path}/aug_data/line_new.txt\", \"r\").readlines()\n","for line in lines_aug:\n","    if line[0] == \"#\":\n","        continue\n","    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n","        lines_aug_list.append(line)\n","\n","np.random.shuffle(lines_aug_list)\n","len(lines_aug_list)"],"metadata":{"id":"Erkhy52dNvTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i=np.random.randint(len(lines_aug_list),size=(int(len(lines_aug_list)/2)))\n","lines_aug_list=np.delete(lines_aug_list,i)"],"metadata":{"id":"u9KVKmwpCBeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iITHuIyb0DaM"},"source":["## Data input pipeline\n","\n","We start building our data input pipeline by first preparing the image paths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhvX_QSz0DaM"},"outputs":[],"source":["# orignal data\n","base_image_path = os.path.join(base_path,\"data\" ,\"lines\")\n","\n","\n","def get_image_paths_and_labels(samples):\n","    paths = []\n","    corrected_samples = []\n","    for (i, file_line) in enumerate(samples):\n","        line_split = file_line.strip()\n","        line_split = line_split.split(\" \")\n","\n","        # Each line split will have this format for the corresponding image:\n","        # part1/part1-part2/part1-part2-part3.png\n","        image_name = line_split[0]\n","        partI = image_name.split(\"-\")[0]\n","        partII = image_name.split(\"-\")[1]\n","        img_path = os.path.join(\n","            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n","        )\n","        if os.path.getsize(img_path):\n","            paths.append(img_path)\n","            corrected_samples.append(file_line.split(\"\\n\")[0])\n","\n","    return paths, corrected_samples\n","\n","img_paths, labels = get_image_paths_and_labels(lines_list)"]},{"cell_type":"code","source":["# augmented data\n","base_image_path = os.path.join(base_path,\"aug_data\" ,\"aug_lines\")\n","\n","\n","def get_image_paths_and_labels(samples):\n","    paths = []\n","    corrected_samples = []\n","    for (i, file_line) in enumerate(samples):\n","        base_image_path = os.path.join(base_path,\"aug_data\" ,\"aug_lines\")\n","        line_split=file_line.strip()\n","        line_split = line_split.split(\" \")\n","        image_name=line_split[0]\n","        img_path = os.path.join(base_image_path, image_name)\n","        if os.path.getsize(img_path):\n","            paths.append(img_path)\n","            corrected_samples.append(file_line.split(\"\\n\")[0])\n","\n","    return paths, corrected_samples\n","\n","img_paths_aug, labels_aug = get_image_paths_and_labels(lines_aug_list)"],"metadata":{"id":"sjZOrWYFdPMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_paths=np.array(img_paths+img_paths_aug)\n","labels=np.array(labels+labels_aug)\n","\n","r_indexes = np.arange(len(img_paths))\n","np.random.shuffle(r_indexes)\n","\n","img_paths=img_paths[r_indexes]\n","labels=labels[r_indexes]\n","\n","len(img_paths)"],"metadata":{"id":"cM2mR_reiCl6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bAZ0fuu0DaL"},"source":["We will split the dataset into three subsets with a 90:5:5 ratio (train:validation:test)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDJqmUZK0DaL"},"outputs":[],"source":["split_idx = int(0.7 * len(img_paths))\n","\n","train_img_paths = img_paths[:split_idx]\n","train_labels= labels[:split_idx]\n","\n","test_img_paths = img_paths[split_idx:]\n","test_labels=labels[split_idx:]\n","\n","val_split_idx = int(0.5 * len(test_img_paths))\n","\n","validation_img_paths = test_img_paths[:val_split_idx]\n","validation_labels= test_labels[:val_split_idx]\n","\n","test_img_paths = test_img_paths[val_split_idx:]\n","test_labels= test_labels[val_split_idx:]\n","\n","\n","assert len(img_paths) == len(train_img_paths) + len(validation_img_paths) + len(test_img_paths)\n","\n","print(f\"Total training samples: {len(train_img_paths)}\")\n","print(f\"Total validation samples: {len(validation_img_paths)}\")\n","print(f\"Total test samples: {len(test_img_paths)}\")"]},{"cell_type":"markdown","metadata":{"id":"dGr39Yui0DaM"},"source":["Then we prepare the ground-truth labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi1gtbOo0DaN"},"outputs":[],"source":["# Find maximum length and the size of the vocabulary in the training data.\n","train_labels_cleaned = []\n","characters = set()\n","max_len = 0\n","\n","for label in train_labels:\n","    label = label.split(\" \")[-1].strip()\n","    label=label.replace(\"|\",\" \")\n","    for char in label:\n","        characters.add(char)\n","\n","    max_len = max(max_len, len(label))\n","    train_labels_cleaned.append(label)\n","\n","characters = sorted(list(characters))\n","\n","print(\"Maximum length: \", max_len)\n","print(\"Vocab size: \", len(characters))\n","\n","# Check some label samples.\n","train_labels_cleaned[:10]"]},{"cell_type":"markdown","metadata":{"id":"lyQBZnqz0DaN"},"source":["Now we clean the validation and the test labels as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csqr2q850DaN"},"outputs":[],"source":["\n","def clean_labels(labels):\n","    cleaned_labels = []\n","    for label in labels:\n","        label = label.split(\" \")[-1].strip()\n","        label=label.replace(\"|\",\" \")\n","        cleaned_labels.append(label)\n","    return cleaned_labels\n","\n","\n","validation_labels_cleaned = clean_labels(validation_labels)\n","test_labels_cleaned = clean_labels(test_labels)"]},{"cell_type":"markdown","metadata":{"id":"97Wl_ggL0DaN"},"source":["### Building the character vocabulary\n","\n","Keras provides different preprocessing layers to deal with different modalities of data.\n","[This guide](https://keras.io/guides/preprocessing_layers/) provides a comprehensive introduction.\n","Our example involves preprocessing labels at the character\n","level. This means that if there are two labels, e.g. \"cat\" and \"dog\", then our character\n","vocabulary should be {a, c, d, g, o, t} (without any special tokens). We use the\n","[`StringLookup`](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup/)\n","layer for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuEDbdPW0DaO"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","# Mapping characters to integers.\n","char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n","\n","# Mapping integers back to original characters.\n","num_to_char = StringLookup(\n","    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"T9lc2wM60DaO"},"source":["### Resizing images without distortion\n","\n","Instead of square images, many OCR models work with rectangular images. This will become\n","clearer in a moment when we will visualize a few samples from the dataset. While\n","aspect-unaware resizing square images does not introduce a significant amount of\n","distortion this is not the case for rectangular images. But resizing images to a uniform\n","size is a requirement for mini-batching. So we need to perform our resizing such that\n","the following criteria are met:\n","\n","* Aspect ratio is preserved.\n","* Content of the images is not affected."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTAVv8QJ0DaO"},"outputs":[],"source":["def distortion_free_resize(image, img_size):\n","    w, h = img_size\n","    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n","\n","    # Check tha amount of padding needed to be done.\n","    pad_height = h - tf.shape(image)[0]\n","    pad_width = w - tf.shape(image)[1]\n","\n","    # Only necessary if you want to do same amount of padding on both sides.\n","    if pad_height % 2 != 0:\n","        height = pad_height // 2\n","        pad_height_top = height + 1\n","        pad_height_bottom = height\n","    else:\n","        pad_height_top = pad_height_bottom = pad_height // 2\n","\n","    if pad_width % 2 != 0:\n","        width = pad_width // 2\n","        pad_width_left = width + 1\n","        pad_width_right = width\n","    else:\n","        pad_width_left = pad_width_right = pad_width // 2\n","\n","    image = tf.pad(\n","        image,\n","        paddings=[\n","            [pad_height_top, pad_height_bottom],\n","            [pad_width_left, pad_width_right],\n","            [0, 0],\n","        ],\n","    )\n","\n","    image = tf.transpose(image, perm=[1, 0, 2])\n","    image = tf.image.flip_left_right(image)\n","    return image\n"]},{"cell_type":"markdown","metadata":{"id":"ODFegPTJ0DaP"},"source":["### Putting the utilities together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aerzWmQk0DaP"},"outputs":[],"source":["batch_size = 24\n","padding_token = 99\n","image_width = 512\n","image_height = 128\n","\n","\n","def preprocess_image(image_path, img_size=(image_width, image_height)):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_png(image, 1)\n","    image = distortion_free_resize(image, img_size)\n","    image = tf.cast(image, tf.float32) / 255.0\n","    return image\n","\n","\n","def vectorize_label(label):\n","    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n","    length = tf.shape(label)[0]\n","    pad_amount = max_len - length\n","    label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=padding_token)\n","    return label\n","\n","\n","def process_images_labels(image_path, label):\n","    image = preprocess_image(image_path)\n","    label = vectorize_label(label)\n","    return {\"image\": image, \"label\": label}\n","\n","\n","def prepare_dataset(image_paths, labels):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(\n","        process_images_labels, num_parallel_calls=AUTOTUNE\n","    )\n","    return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)\n"]},{"cell_type":"markdown","metadata":{"id":"H-SrSXS10DaP"},"source":["## Prepare `tf.data.Dataset` objects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYJWZfMy0DaP"},"outputs":[],"source":["train_ds = prepare_dataset(train_img_paths, train_labels_cleaned)\n","validation_ds = prepare_dataset(validation_img_paths, validation_labels_cleaned)\n","test_ds = prepare_dataset(test_img_paths, test_labels_cleaned)"]},{"cell_type":"markdown","metadata":{"id":"aEqLhzkp0DaQ"},"source":["## Visualize a few samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0CST5a10DaQ"},"outputs":[],"source":["for data in train_ds.take(1):\n","    images, labels = data[\"image\"], data[\"label\"]\n","\n","    _, ax = plt.subplots(2, 2, figsize=(15, 8))\n","\n","    for i in range(4):\n","        img = images[i]\n","        img = tf.image.flip_left_right(img)\n","        img = tf.transpose(img, perm=[1, 0, 2])\n","        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","        img = img[:, :, 0]\n","\n","        # Gather indices where label!= padding_token.\n","        label = labels[i]\n","        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n","        # Convert to string.\n","        label = tf.strings.reduce_join(num_to_char(indices))\n","        label = label.numpy().decode(\"utf-8\")\n","\n","        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n","        ax[i // 2, i % 2].set_title(label)\n","        ax[i // 2, i % 2].axis(\"off\")\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yJuGrGG80DaQ"},"source":["You will notice that the content of original image is kept as faithful as possible and has\n","been padded accordingly."]},{"cell_type":"markdown","metadata":{"id":"gkHZOWCh0DaQ"},"source":["## Model\n","\n","Our model will use the CTC loss as an endpoint layer. For a detailed understanding of the\n","CTC loss, refer to [this post](https://distill.pub/2017/ctc/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plzcR0sS0DaQ"},"outputs":[],"source":["class CTCLayer(keras.layers.Layer):\n","\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = keras.backend.ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n","        self.add_loss(loss)\n","\n","        # At test time, just return the computed predictions.\n","        return y_pred\n","\n","\n","def build_model():\n","    # Inputs to the model\n","    input_img = keras.Input(shape=(image_width, image_height, 1), name=\"image\")\n","    labels = keras.layers.Input(name=\"label\", shape=None)\n","\n","    # Conv blocks\n","    x = keras.layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\",)(input_img)\n","    x = keras.layers.Conv2D(128,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\",)(x)\n","    x = keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n","    x = keras.layers.Conv2D(256,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv3\",)(x)\n","    x = keras.layers.Dropout(0.2)(x)\n","    x = keras.layers.Conv2D(1024,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv4\",)(x)\n","    x = keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n","    x = keras.layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv5\",)(x)\n","    new_shape = ((image_width // 4), (image_height // 4) * 64)\n","    x = keras.layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n","    x = keras.layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n","    x = keras.layers.Dropout(0.2)(x)\n","\n","    # RNNs.\n","    x = keras.layers.Bidirectional(\n","        keras.layers.LSTM(1024, return_sequences=True, dropout=0.3)\n","    )(x)\n","    x = keras.layers.Bidirectional(\n","        keras.layers.LSTM(512, return_sequences=True, dropout=0.25)\n","    )(x)\n","    x = keras.layers.Bidirectional(\n","        keras.layers.LSTM(64, return_sequences=True, dropout=0.2)\n","    )(x)\n","\n","    x = keras.layers.Dense(len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\")(x)\n","\n","    # Add CTC layer for calculating CTC loss at each step.\n","    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n","\n","    # Define the model.\n","    model = keras.models.Model(\n","        inputs=[input_img, labels], outputs=output, name=\"handwriting_recognizer\"\n","    )\n","    # Optimizer.\n","    opt = keras.optimizers.Adam()\n","    # Compile the model and return.\n","    model.compile(optimizer=opt)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"IuPLB0oc0DaT"},"source":["## Training\n","\n","Now we are ready to kick off model training."]},{"cell_type":"code","source":["model = build_model()\n","# model.load_weights(base_path+\"/handwriting.h5\")\n","model.summary()"],"metadata":{"id":"-rA1_Zph4cmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHOHhMBjWX_N"},"outputs":[],"source":["# Train the model.\n","checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(base_path+\"/handwriting.h5\", save_best_only=True)\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,monitor='val_loss',restore_best_weights=True)\n","epochs = 50\n","\n","history = model.fit(\n","    train_ds,\n","    validation_data=validation_ds,\n","    epochs=epochs,\n","    callbacks=[checkpoint_cb,early_stopping_cb],\n",")"]},{"cell_type":"code","source":["pd.DataFrame(history.history).plot(figsize=(8, 5))\n","plt.grid(True)\n","plt.gca().set_ylim(0, 1)\n","plt.show()"],"metadata":{"id":"TmgYXzKnE9wq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4PWS-J1K0DaR"},"source":["## Evaluation metric\n","\n","[Edit Distance](https://en.wikipedia.org/wiki/Edit_distance)\n","is the most widely used metric for evaluating OCR models. In this section, we will\n","implement it and use it as a callback to monitor our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hJXa4Qd0DaR"},"outputs":[],"source":["def calculate_edit_distance(labels, predictions):\n","    # Get a single batch and convert its labels to sparse tensors.\n","    saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n","\n","    # Make predictions and convert them to sparse tensors.\n","    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n","    predictions_decoded = keras.backend.ctc_decode(\n","        predictions, input_length=input_len, greedy=True\n","    )[0][0][:, :max_len]\n","    sparse_predictions = tf.cast(\n","        tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n","    )\n","\n","    # Compute individual edit distances and average them out.\n","    edit_distances = tf.edit_distance(\n","        sparse_predictions, saprse_labels, normalize=False\n","    )\n","    return tf.reduce_mean(edit_distances)"]},{"cell_type":"markdown","metadata":{"id":"MQKo1LVN0DaT"},"source":["## Inference"]},{"cell_type":"code","source":["model=build_model()\n","model.load_weights(base_path+\"/handwriting.h5\")\n","prediction_model = keras.models.Model(\n","     model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",")"],"metadata":{"id":"3V5EgOX5V_Q2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALW0259Z0DaT"},"outputs":[],"source":["# A utility function to decode the output of the network.\n","def decode_batch_predictions(pred):\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n","    # Use greedy search. For complex tasks, you can use beam search.\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n","        :, :max_len\n","    ]\n","    # Iterate over the results and get back the text.\n","    output_text = []\n","    for res in results:\n","        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n","        output_text.append(res)\n","    return output_text\n","\n","\n","#  Let's check results on some test samples.\n","for batch in test_ds.take(1):\n","    # print(batch[\"label\"])\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","    print(\"len is : \",len(batch))\n","    print(\"batch img shape: \",batch_images[1].shape)\n","    # print(batch)\n","    _, ax = plt.subplots(2, 2, figsize=(15, 8))\n","\n","    preds = prediction_model.predict(batch_images)\n","    pred_texts = decode_batch_predictions(preds)\n","\n","    for i in range(4):\n","        img = batch_images[i]\n","        img = tf.image.flip_left_right(img)\n","        img = tf.transpose(img, perm=[1, 0, 2])\n","        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","        img = img[:, :, 0]\n","\n","        label = batch_labels[i]\n","        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n","        # Convert to string.\n","        label = tf.strings.reduce_join(num_to_char(indices))\n","        label = label.numpy().decode(\"utf-8\")\n","\n","        title = f\"Prediction: {pred_texts[i]} \\n\\n Original : {label}\"\n","        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n","        ax[i // 2, i % 2].set_title(title)\n","        ax[i // 2, i % 2].axis(\"off\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGtQI1IDyLiG"},"outputs":[],"source":["# Edit Distance\n","\n","ed_sum = 0\n","count=0\n","\n","for batch in test_ds.take(len(test_img_paths)//batch_size + 1):\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","\n","    preds = model.predict(batch)\n","    ed_sum+=calculate_edit_distance(batch_labels,preds)\n","    count+=1\n","\n","\n","print(\"\\nEdit Distance : \",ed_sum/count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DokrqTe5uaU"},"outputs":[],"source":["img_path = base_path+'/test1.png'\n","def prepare_dataset_custom(img_paths_3):\n","    dataset = tf.data.Dataset.from_tensor_slices((img_paths_3)).map(\n","        preprocess_image, num_parallel_calls=AUTOTUNE\n","    )\n","    return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)\n","img_paths_3 = [img_path]\n","custom_ds = prepare_dataset_custom(img_paths_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ujqdspc_cLF"},"outputs":[],"source":["for batch in custom_ds.take(1):\n","    print(\"len is : \",len(batch))\n","    print(\"batch img shape: \",batch[0].shape)\n","\n","    preds = prediction_model.predict(batch)\n","    pred_texts = decode_batch_predictions(preds)\n","    print(pred_texts)\n","\n","    for i in range(1):\n","        img = batch[i]\n","        img = tf.image.flip_left_right(img)\n","        img = tf.transpose(img, perm=[1, 0, 2])\n","        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","        img = img[:, :, 0]\n","\n","        title = f\"Prediction: {pred_texts[i]}\"\n","        plt.imshow(img, cmap=\"gray\")\n","        plt.title(title)\n","        plt.axis(\"off\")\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Fine Tuning"],"metadata":{"id":"x-NWIkwZWeFd"}},{"cell_type":"markdown","source":["### Import data and preprocessing"],"metadata":{"id":"-hz5eJEWWf9Q"}},{"cell_type":"code","source":["df=pd.read_csv(base_path+\"/custom_data/annot.csv\")\n","df_aug=pd.read_csv(base_path+\"/custom_data/annot1.csv\")\n","\n","df=pd.concat([df,df_aug])\n","df = df.sample(frac = 1,ignore_index=True)\n","df.head()"],"metadata":{"id":"9W9taf1fWdzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path=base_path+\"/custom_data/images/\"\n","def get_paths(x):\n","  return data_path+x\n","\n","df['images']=df['images'].apply(lambda x:get_paths(x))\n","df.head()"],"metadata":{"id":"C0Nojgu5WnIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train,X_test,y_train,y_test=train_test_split(df['images'],df['label'],test_size=0.2)\n","X_val,X_test,y_val,y_test=train_test_split(X_test,y_test,test_size=0.5)\n","\n","print(\"Training Size:\",len(X_train))\n","print(\"Validation Size:\",len(X_val))\n","print(\"Testing Size:\",len(X_test))"],"metadata":{"id":"Oqli1QtkWpqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = prepare_dataset(X_train, y_train)\n","validation_ds = prepare_dataset(X_val, y_val)\n","test_ds = prepare_dataset(X_test, y_test)"],"metadata":{"id":"059GZuL-Wpns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Performance before transfer learning"],"metadata":{"id":"tgq3MaF_hNwN"}},{"cell_type":"code","source":["ed_sum = 0\n","count=0\n","\n","for batch in train_ds.take(len(test_img_paths)//batch_size + 1):\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","\n","    preds = model.predict(batch)\n","    ed_sum+=calculate_edit_distance(batch_labels,preds)\n","    count+=1\n","\n","\n","print(\"\\nEdit Distance : \",ed_sum/count)"],"metadata":{"id":"Ae5_Ac5qhNkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ed_sum = 0\n","count=0\n","\n","for batch in test_ds.take(len(test_img_paths)//batch_size + 1):\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","\n","    preds = model.predict(batch)\n","    ed_sum+=calculate_edit_distance(batch_labels,preds)\n","    count+=1\n","\n","\n","print(\"\\nEdit Distance : \",ed_sum/count)"],"metadata":{"id":"yHDa2FKRhh98"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize Data"],"metadata":{"id":"EGxxceLxW2kX"}},{"cell_type":"code","source":["for data in train_ds.take(1):\n","    images, labels = data[\"image\"], data[\"label\"]\n","\n","    _, ax = plt.subplots(2, 2, figsize=(15, 8))\n","\n","    for i in range(4):\n","        img = images[i]\n","        img = tf.image.flip_left_right(img)\n","        img = tf.transpose(img, perm=[1, 0, 2])\n","        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","        img = img[:, :, 0]\n","\n","        # Gather indices where label!= padding_token.\n","        label = labels[i]\n","        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n","        # Convert to string.\n","        label = tf.strings.reduce_join(num_to_char(indices))\n","        label = label.numpy().decode(\"utf-8\")\n","\n","        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n","        ax[i // 2, i % 2].set_title(label)\n","        ax[i // 2, i % 2].axis(\"off\")\n","\n","\n","plt.show()"],"metadata":{"id":"nbD_3oLPWpk2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fine Tuning"],"metadata":{"id":"886xy_tXW6Ar"}},{"cell_type":"code","source":["model=build_model()\n","model.load_weights(base_path+\"/handwriting.h5\")"],"metadata":{"id":"l1vCIDoEWpig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model.\n","checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(base_path+\"/fine_tuned.h5\", save_best_only=True)\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,monitor='val_loss',restore_best_weights=True)\n","epochs = 50\n","\n","history = model.fit(\n","    train_ds,\n","    validation_data=validation_ds,\n","    epochs=epochs,\n","    callbacks=[early_stopping_cb,checkpoint_cb],\n",")"],"metadata":{"id":"d0k8xoSdWpf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(history.history).plot(figsize=(8, 5))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"CZWSXjezgp5h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"DqEdmMZfXC5r"}},{"cell_type":"code","source":["model=build_model()\n","model.load_weights(base_path+\"/fine_tuned.h5\")"],"metadata":{"id":"fnvPbct5BN13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_model = keras.models.Model(\n","     model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",")"],"metadata":{"id":"0QPRxo8oWpab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A utility function to decode the output of the network.\n","def decode_batch_predictions(pred):\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n","    # Use greedy search. For complex tasks, you can use beam search.\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n","        :, :max_len\n","    ]\n","    # Iterate over the results and get back the text.\n","    output_text = []\n","    for res in results:\n","        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n","        output_text.append(res)\n","    return output_text\n","\n","\n","#  Let's check results on some test samples.\n","for batch in test_ds.take(1):\n","    # print(batch[\"label\"])\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","    print(\"len is : \",len(batch))\n","    print(\"batch img shape: \",batch_images[1].shape)\n","    # print(batch)\n","    _, ax = plt.subplots(2, 2, figsize=(15, 8))\n","\n","    preds = model.predict(batch)\n","    pred_texts = decode_batch_predictions(preds)\n","\n","    for i in range(4):\n","        img = batch_images[i]\n","        img = tf.image.flip_left_right(img)\n","        img = tf.transpose(img, perm=[1, 0, 2])\n","        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","        img = img[:, :, 0]\n","\n","        label = batch_labels[i]\n","        indices = tf.gather(label, tf.where(tf.math.not_equal(label, padding_token)))\n","        # Convert to string.\n","        label = tf.strings.reduce_join(num_to_char(indices))\n","        label = label.numpy().decode(\"utf-8\")\n","\n","        title = f\"Prediction: {pred_texts[i]} \\n\\n Original : {label}\"\n","        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n","        ax[i // 2, i % 2].set_title(title)\n","        ax[i // 2, i % 2].axis(\"off\")\n","\n","plt.show()"],"metadata":{"id":"C0Z94t0zWpWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ed_sum = 0\n","count=0\n","\n","for batch in test_ds.take(len(X_test)//batch_size + 1):\n","    batch_images,batch_labels = batch[\"image\"],batch[\"label\"]\n","\n","\n","    preds = model.predict(batch)\n","    ed_sum+=calculate_edit_distance(batch_labels,preds)\n","    count+=1\n","\n","\n","print(\"\\nEdit Distance : \",ed_sum/count)"],"metadata":{"id":"SgzFS19NXG_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1h7kyVbwLcHT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predictions"],"metadata":{"id":"7n0UMNUgXR4y"}},{"cell_type":"code","source":["df_med=pd.read_csv(base_path+\"/medicines.csv\")\n","df_med.drop('id',axis=1,inplace=True)"],"metadata":{"id":"guKl73hrXG9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_dataset_custom(img_paths_3):\n","    dataset = tf.data.Dataset.from_tensor_slices((img_paths_3)).map(\n","        preprocess_image, num_parallel_calls=AUTOTUNE\n","    )\n","    return dataset.batch(batch_size).cache().prefetch(AUTOTUNE)\n","\n","def make_pred(img_path):\n","  img_paths = [img_path]\n","  custom_ds = prepare_dataset_custom(img_paths)\n","  for batch in custom_ds.take(1):\n","    preds = prediction_model.predict(batch)\n","    pred_texts = decode_batch_predictions(preds)\n","\n","    img = batch[0]\n","    img = tf.image.flip_left_right(img)\n","    img = tf.transpose(img, perm=[1, 0, 2])\n","    img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n","    img = img[:, :, 0]\n","\n","    title = f\"Prediction: {pred_texts[0]}\"\n","    plt.imshow(img, cmap=\"gray\")\n","    plt.title(title)\n","    plt.axis(\"off\")\n","\n","  plt.show()\n","  return pred_texts[0]"],"metadata":{"id":"zvLf3gK_XG7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_medicine_info(pred_texts):\n","  collection = df_med['name']\n","  med=process.extract(pred_texts, collection, scorer=fuzz.token_sort_ratio)\n","  med=med[0]\n","  if med[1]<80:\n","    print(\"Medicine not found\")\n","  else:\n","    print(df_med.iloc[med[2]])\n","    return df_med.iloc[med[2]].to_dict()"],"metadata":{"id":"R_0WDHtMXG1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test Image 1\n","img_path1 = base_path+'/test1.png'\n","pred_texts1=make_pred(img_path1)\n","medicine1=extract_medicine_info(pred_texts1)"],"metadata":{"id":"BTTQkIB0XG4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(medicine1)"],"metadata":{"id":"jUN2oej1-xap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test Image 2\n","img_path2 = base_path+'/test2.png'\n","pred_texts2=make_pred(img_path2)\n","medicine2=extract_medicine_info(pred_texts2)"],"metadata":{"id":"xzfeovc39qZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(medicine2)"],"metadata":{"id":"AogNENZC9sSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test Image 3\n","img_path3 = base_path+'/test3.png'\n","pred_texts3=make_pred(img_path3)\n","medicine3=extract_medicine_info(pred_texts3)"],"metadata":{"id":"MZ_DushGAEEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(medicine3)"],"metadata":{"id":"iiptWViSAFP3"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["sqR-fSou0DaI","CkUECYg40DaK","iITHuIyb0DaM","H-SrSXS10DaP","aEqLhzkp0DaQ","gkHZOWCh0DaQ","IuPLB0oc0DaT","4PWS-J1K0DaR"],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}