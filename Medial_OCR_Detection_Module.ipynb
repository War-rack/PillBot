{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["9nSI0rwoCcrw"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GCFoGiI57dI","executionInfo":{"status":"ok","timestamp":1740853730650,"user_tz":-330,"elapsed":91,"user":{"displayName":"Vanshika Bhargava","userId":"05252072017388527561"}},"outputId":"e6e2541c-d7d2-4019-afee-b12aef68b080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive'\n","/content\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/SakuraRiven/EAST.git"],"metadata":{"id":"z45fM0SMvaGL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740899593642,"user_tz":-330,"elapsed":1128,"user":{"displayName":"Vanshika Bhargava","userId":"05252072017388527561"}},"outputId":"fa40cef6-d85a-4fa8-dfa5-942de06e96d7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'EAST'...\n","remote: Enumerating objects: 78, done.\u001b[K\n","remote: Counting objects: 100% (40/40), done.\u001b[K\n","remote: Compressing objects: 100% (11/11), done.\u001b[K\n","remote: Total 78 (delta 33), reused 29 (delta 29), pack-reused 38 (from 1)\u001b[K\n","Receiving objects: 100% (78/78), 131.72 KiB | 1.20 MiB/s, done.\n","Resolving deltas: 100% (43/43), done.\n"]}]},{"cell_type":"code","source":["!pip install -q lanms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykt_irjTA0oW","executionInfo":{"status":"ok","timestamp":1711469148965,"user_tz":-330,"elapsed":63552,"user":{"displayName":"Anngela Roy","userId":"04086865696182545290"}},"outputId":"b0d427f4-e162-426c-aea2-f309cdd010fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/973.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/973.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/973.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.4/973.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for lanms (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["%cd EAST"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmuKf3isBtDP","executionInfo":{"status":"ok","timestamp":1711469148966,"user_tz":-330,"elapsed":40,"user":{"displayName":"Anngela Roy","userId":"04086865696182545290"}},"outputId":"322ff918-84fc-4b1c-e09e-34efba90de40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/EAST\n"]}]},{"cell_type":"code","source":["!CUDA_VISIBLE_DEVICES=0 python eval.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBAwxOZo6cHX","executionInfo":{"status":"ok","timestamp":1711469171477,"user_tz":-330,"elapsed":22543,"user":{"displayName":"Anngela Roy","userId":"04086865696182545290"}},"outputId":"0b885f14-d11a-4779-e85d-21cedefd071a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/EAST/eval.py\", line 40, in <module>\n","    eval_model(model_name, test_img_path, submit_path)\n","  File \"/content/EAST/eval.py\", line 18, in eval_model\n","    model.load_state_dict(torch.load(model_name))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 998, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 445, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 426, in __init__\n","    super().__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: './pths/east_vgg16.pth'\n"]}]},{"cell_type":"markdown","source":["#EAST"],"metadata":{"id":"9nSI0rwoCcrw"}},{"cell_type":"markdown","source":["##MODEL"],"metadata":{"id":"Aeu_XfPXDCyT"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","import torch.nn.functional as F\n","import math"],"metadata":{"id":"Lso2mmeI61lG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Feature Extractor Stem"],"metadata":{"id":"8YD5ikOqHnl5"}},{"cell_type":"code","source":["cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n","\n","\n","def make_layers(cfg, batch_norm=False):\n","\tlayers = []\n","\tin_channels = 3\n","\tfor v in cfg:\n","\t\tif v == 'M':\n","\t\t\tlayers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","\t\telse:\n","\t\t\tconv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","\t\t\tif batch_norm:\n","\t\t\t\tlayers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","\t\t\telse:\n","\t\t\t\tlayers += [conv2d, nn.ReLU(inplace=True)]\n","\t\t\tin_channels = v\n","\treturn nn.Sequential(*layers)"],"metadata":{"id":"wodl_fJ-IgXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["VGG Class"],"metadata":{"id":"R02JbYxVETWT"}},{"cell_type":"code","source":["class VGG(nn.Module):\n","\tdef __init__(self, features):\n","\t\tsuper(VGG, self).__init__()\n","\t\tself.features = features\n","\t\tself.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","\t\tself.classifier = nn.Sequential(\n","\t\t\tnn.Linear(512 * 7 * 7, 4096),\n","\t\t\tnn.ReLU(True),\n","\t\t\tnn.Dropout(),\n","\t\t\tnn.Linear(4096, 4096),\n","\t\t\tnn.ReLU(True),\n","\t\t\tnn.Dropout(),\n","\t\t\tnn.Linear(4096, 1000),\n","\t\t)\n","\n","\t\tfor m in self.modules():\n","\t\t\tif isinstance(m, nn.Conv2d):\n","\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","\t\t\t\tif m.bias is not None:\n","\t\t\t\t\tnn.init.constant_(m.bias, 0)\n","\t\t\telif isinstance(m, nn.BatchNorm2d):\n","\t\t\t\tnn.init.constant_(m.weight, 1)\n","\t\t\t\tnn.init.constant_(m.bias, 0)\n","\t\t\telif isinstance(m, nn.Linear):\n","\t\t\t\tnn.init.normal_(m.weight, 0, 0.01)\n","\t\t\t\tnn.init.constant_(m.bias, 0)\n","\n","\tdef forward(self, x):\n","\t\tx = self.features(x)\n","\t\tx = self.avgpool(x)\n","\t\tx = x.view(x.size(0), -1)\n","\t\tx = self.classifier(x)\n","\t\treturn x"],"metadata":{"id":"C9zSe1hpEOTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feature Extractor"],"metadata":{"id":"mMrR7yVnEZ4e"}},{"cell_type":"code","source":["vgg_path = '/content/drive/MyDrive/EAST/pths/vgg16_bn-6c64b313.pth'"],"metadata":{"id":"hrwCfla2JsKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class extractor(nn.Module):\n","\tdef __init__(self, pretrained):\n","\t\tsuper(extractor, self).__init__()\n","\t\tvgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n","\t\tif pretrained:\n","\t\t\tvgg16_bn.load_state_dict(torch.load(vgg_path))\n","\t\tself.features = vgg16_bn.features\n","\n","\tdef forward(self, x):\n","\t\tout = []\n","\t\tfor m in self.features:\n","\t\t\tx = m(x)\n","\t\t\tif isinstance(m, nn.MaxPool2d):\n","\t\t\t\tout.append(x)\n","\t\treturn out[1:]"],"metadata":{"id":"W4ssdGhwEYQu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Feature Merging Branch"],"metadata":{"id":"h-4TQZUuHvta"}},{"cell_type":"code","source":["class merge(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper(merge, self).__init__()\n","\n","\t\tself.conv1 = nn.Conv2d(1024, 128, 1)\n","\t\tself.bn1 = nn.BatchNorm2d(128)\n","\t\tself.relu1 = nn.ReLU()\n","\t\tself.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n","\t\tself.bn2 = nn.BatchNorm2d(128)\n","\t\tself.relu2 = nn.ReLU()\n","\n","\t\tself.conv3 = nn.Conv2d(384, 64, 1)\n","\t\tself.bn3 = nn.BatchNorm2d(64)\n","\t\tself.relu3 = nn.ReLU()\n","\t\tself.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n","\t\tself.bn4 = nn.BatchNorm2d(64)\n","\t\tself.relu4 = nn.ReLU()\n","\n","\t\tself.conv5 = nn.Conv2d(192, 32, 1)\n","\t\tself.bn5 = nn.BatchNorm2d(32)\n","\t\tself.relu5 = nn.ReLU()\n","\t\tself.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n","\t\tself.bn6 = nn.BatchNorm2d(32)\n","\t\tself.relu6 = nn.ReLU()\n","\n","\t\tself.conv7 = nn.Conv2d(32, 32, 3, padding=1)\n","\t\tself.bn7 = nn.BatchNorm2d(32)\n","\t\tself.relu7 = nn.ReLU()\n","\n","\t\tfor m in self.modules():\n","\t\t\tif isinstance(m, nn.Conv2d):\n","\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","\t\t\t\tif m.bias is not None:\n","\t\t\t\t\tnn.init.constant_(m.bias, 0)\n","\t\t\telif isinstance(m, nn.BatchNorm2d):\n","\t\t\t\tnn.init.constant_(m.weight, 1)\n","\t\t\t\tnn.init.constant_(m.bias, 0)\n","\n","\tdef forward(self, x):\n","\t\ty = F.interpolate(x[3], scale_factor=2, mode='bilinear', align_corners=True)\n","\t\ty = torch.cat((y, x[2]), 1)\n","\t\ty = self.relu1(self.bn1(self.conv1(y)))\n","\t\ty = self.relu2(self.bn2(self.conv2(y)))\n","\n","\t\ty = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n","\t\ty = torch.cat((y, x[1]), 1)\n","\t\ty = self.relu3(self.bn3(self.conv3(y)))\n","\t\ty = self.relu4(self.bn4(self.conv4(y)))\n","\n","\t\ty = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n","\t\ty = torch.cat((y, x[0]), 1)\n","\t\ty = self.relu5(self.bn5(self.conv5(y)))\n","\t\ty = self.relu6(self.bn6(self.conv6(y)))\n","\n","\t\ty = self.relu7(self.bn7(self.conv7(y)))\n","\t\treturn y"],"metadata":{"id":"J-yZ0IqeEm5d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Output"],"metadata":{"id":"83rUBxW3I1Xl"}},{"cell_type":"code","source":["class output(nn.Module):\n","\tdef __init__(self, scope=512):\n","\t\tsuper(output, self).__init__()\n","\t\tself.conv1 = nn.Conv2d(32, 1, 1)\n","\t\tself.sigmoid1 = nn.Sigmoid()\n","\t\tself.conv2 = nn.Conv2d(32, 4, 1)\n","\t\tself.sigmoid2 = nn.Sigmoid()\n","\t\tself.conv3 = nn.Conv2d(32, 1, 1)\n","\t\tself.sigmoid3 = nn.Sigmoid()\n","\t\tself.scope = 512\n","\t\tfor m in self.modules():\n","\t\t\tif isinstance(m, nn.Conv2d):\n","\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","\t\t\t\tif m.bias is not None:\n","\t\t\t\t\tnn.init.constant_(m.bias, 0)\n","\n","\tdef forward(self, x):\n","\t\tscore = self.sigmoid1(self.conv1(x))\n","\t\tloc   = self.sigmoid2(self.conv2(x)) * self.scope\n","\t\tangle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n","\t\tgeo   = torch.cat((loc, angle), 1)\n","\t\treturn score, geo"],"metadata":{"id":"9hbpLUHHIIXI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###EAST"],"metadata":{"id":"hWpa1T2_JfED"}},{"cell_type":"code","source":["class EAST(nn.Module):\n","\tdef __init__(self, pretrained=True):\n","\t\tsuper(EAST, self).__init__()\n","\t\tself.extractor = extractor(pretrained)\n","\t\tself.merge     = merge()\n","\t\tself.output    = output()\n","\n","\tdef forward(self, x):\n","\t\treturn self.output(self.merge(self.extractor(x)))\n"],"metadata":{"id":"f-FFKCuOI9q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = EAST()\n","x = torch.randn(1, 3, 256, 256)\n","score, geo = m(x)\n","print(score.shape)\n","print(geo.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"4_3p2FV1JgKg","executionInfo":{"status":"error","timestamp":1740853722077,"user_tz":-330,"elapsed":3749,"user":{"displayName":"Vanshika Bhargava","userId":"05252072017388527561"}},"outputId":"a621d068-1797-4425-9452-f41d11ab8ea4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-942adeb36f6c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  vgg16_bn.load_state_dict(torch.load(vgg_path))\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/EAST/pths/vgg16_bn-6c64b313.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-690d17f0a0f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEAST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-5e9a4515e323>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained)\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEAST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-942adeb36f6c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained)\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mvgg16_bn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         \u001b[0mvgg16_bn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16_bn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/EAST/pths/vgg16_bn-6c64b313.pth'"]}]},{"cell_type":"markdown","source":["#Dataset"],"metadata":{"id":"Lp9lxUCdOJQf"}},{"cell_type":"code","source":["import torch\n","import os\n","from torchvision import transforms\n","from PIL import Image, ImageDraw\n","import numpy as np\n","!pip install -q lanms\n","import lanms"],"metadata":{"id":"nYfDi69zONRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"tNjHs302TdDq"}},{"cell_type":"code","source":["def get_rotate_mat(theta):\n","\t'''positive theta value means rotate clockwise'''\n","\treturn np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])"],"metadata":{"id":"vET99oHRTO59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resize_img(img):\n","\t'''resize image to be divisible by 32\n","\t'''\n","\tw, h = img.size\n","\tresize_w = w\n","\tresize_h = h\n","\n","\tresize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\n","\tresize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\n","\timg = img.resize((resize_w, resize_h), Image.BILINEAR)\n","\tratio_h = resize_h / h\n","\tratio_w = resize_w / w\n","\n","\treturn img, ratio_h, ratio_w"],"metadata":{"id":"itwnppmEPFAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_pil(img):\n","\t'''convert PIL Image to torch.Tensor\n","\t'''\n","\tt = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n","\treturn t(img).unsqueeze(0)"],"metadata":{"id":"5oINsP28PFrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_valid_poly(res, score_shape, scale):\n","\t'''check if the poly in image scope\n","\tInput:\n","\t\tres        : restored poly in original image\n","\t\tscore_shape: score map shape\n","\t\tscale      : feature map -> image\n","\tOutput:\n","\t\tTrue if valid\n","\t'''\n","\tcnt = 0\n","\tfor i in range(res.shape[1]):\n","\t\tif res[0,i] < 0 or res[0,i] >= score_shape[1] * scale or \\\n","           res[1,i] < 0 or res[1,i] >= score_shape[0] * scale:\n","\t\t\tcnt += 1\n","\treturn True if cnt <= 1 else False"],"metadata":{"id":"oYRKUDDaPbuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def restore_polys(valid_pos, valid_geo, score_shape, scale=4):\n","\t'''restore polys from feature maps in given positions\n","\tInput:\n","\t\tvalid_pos  : potential text positions <numpy.ndarray, (n,2)>\n","\t\tvalid_geo  : geometry in valid_pos <numpy.ndarray, (5,n)>\n","\t\tscore_shape: shape of score map\n","\t\tscale      : image / feature map\n","\tOutput:\n","\t\trestored polys <numpy.ndarray, (n,8)>, index\n","\t'''\n","\tpolys = []\n","\tindex = []\n","\tvalid_pos *= scale\n","\td = valid_geo[:4, :] # 4 x N\n","\tangle = valid_geo[4, :] # N,\n","\n","\tfor i in range(valid_pos.shape[0]):\n","\t\tx = valid_pos[i, 0]\n","\t\ty = valid_pos[i, 1]\n","\t\ty_min = y - d[0, i]\n","\t\ty_max = y + d[1, i]\n","\t\tx_min = x - d[2, i]\n","\t\tx_max = x + d[3, i]\n","\t\trotate_mat = get_rotate_mat(-angle[i])\n","\n","\t\ttemp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\n","\t\ttemp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\n","\t\tcoordidates = np.concatenate((temp_x, temp_y), axis=0)\n","\t\tres = np.dot(rotate_mat, coordidates)\n","\t\tres[0,:] += x\n","\t\tres[1,:] += y\n","\n","\t\tif is_valid_poly(res, score_shape, scale):\n","\t\t\tindex.append(i)\n","\t\t\tpolys.append([res[0,0], res[1,0], res[0,1], res[1,1], res[0,2], res[1,2],res[0,3], res[1,3]])\n","\treturn np.array(polys), index"],"metadata":{"id":"I2vhpCdePe_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_boxes(score, geo, score_thresh=0.9, nms_thresh=0.2):\n","\t'''get boxes from feature map\n","\tInput:\n","\t\tscore       : score map from model <numpy.ndarray, (1,row,col)>\n","\t\tgeo         : geo map from model <numpy.ndarray, (5,row,col)>\n","\t\tscore_thresh: threshold to segment score map\n","\t\tnms_thresh  : threshold in nms\n","\tOutput:\n","\t\tboxes       : final polys <numpy.ndarray, (n,9)>\n","\t'''\n","\tscore = score[0,:,:]\n","\txy_text = np.argwhere(score > score_thresh) # n x 2, format is [r, c]\n","\tif xy_text.size == 0:\n","\t\treturn None\n","\n","\txy_text = xy_text[np.argsort(xy_text[:, 0])]\n","\tvalid_pos = xy_text[:, ::-1].copy() # n x 2, [x, y]\n","\tvalid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]] # 5 x n\n","\tpolys_restored, index = restore_polys(valid_pos, valid_geo, score.shape)\n","\tif polys_restored.size == 0:\n","\t\treturn None\n","\n","\tboxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n","\tboxes[:, :8] = polys_restored\n","\tboxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n","\tboxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thresh)\n","\treturn boxes"],"metadata":{"id":"zhIL9tfmPh9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_ratio(boxes, ratio_w, ratio_h):\n","\t'''refine boxes\n","\tInput:\n","\t\tboxes  : detected polys <numpy.ndarray, (n,9)>\n","\t\tratio_w: ratio of width\n","\t\tratio_h: ratio of height\n","\tOutput:\n","\t\trefined boxes\n","\t'''\n","\tif boxes is None or boxes.size == 0:\n","\t\treturn None\n","\tboxes[:,[0,2,4,6]] /= ratio_w\n","\tboxes[:,[1,3,5,7]] /= ratio_h\n","\treturn np.around(boxes)"],"metadata":{"id":"fSwbB1F7PmnV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def detect(img, model, device):\n","\t'''detect text regions of img using model\n","\tInput:\n","\t\timg   : PIL Image\n","\t\tmodel : detection model\n","\t\tdevice: gpu if gpu is available\n","\tOutput:\n","\t\tdetected polys\n","\t'''\n","\timg, ratio_h, ratio_w = resize_img(img)\n","\twith torch.no_grad():\n","\t\tscore, geo = model(load_pil(img).to(device))\n","\tboxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n","\treturn adjust_ratio(boxes, ratio_w, ratio_h)"],"metadata":{"id":"hZUX3OH1PppG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_boxes(img, boxes):\n","\t'''plot boxes on image\n","\t'''\n","\tif boxes is None:\n","\t\treturn img\n","\n","\tdraw = ImageDraw.Draw(img)\n","\tfor box in boxes:\n","\t\tdraw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0,255,0))\n","\treturn img"],"metadata":{"id":"oF8-I5iGPr-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#EVAL"],"metadata":{"id":"oUY98RRqKclM"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"dGt3X4IVK1eX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/EAST/pths/east_vgg16.pth'"],"metadata":{"id":"eI2xAsKQJ90j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = EAST(False).to(device)\n","model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n","model.eval()"],"metadata":{"id":"i3H7VPKkK97r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#img_path = '/content/drive/MyDrive/ICDAR_2015/test_img/img_106.jpg'\n","img_path = '/content/drive/MyDrive/Medical_ocr/Handwritten_Doctor_Prescriptions_and_Reports/Disha/IMG-20230217-WA0001 - Satwik Tanwar.jpg'"],"metadata":{"id":"OBmlvZ-1UBN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img = Image.open(img_path)\n","img_og = img\n","img, ratio_h, ratio_w = resize_img(img)\n","with torch.no_grad():\n","  score, geo = model(load_pil(img).to(device))\n","boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n","boxes = adjust_ratio(boxes, ratio_w, ratio_h)\n","plot_img = plot_boxes(img, boxes)"],"metadata":{"id":"Z_loipMuUNjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"0uyhNaYjMg2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(plot_img)"],"metadata":{"id":"-wT9k_V_ZMLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(img_og)"],"metadata":{"id":"vBvLEmb0bgNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B3xORXS1clmN"},"execution_count":null,"outputs":[]}]}